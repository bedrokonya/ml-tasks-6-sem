{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> \n",
    "\n",
    "* Дедлайн **DD MM 2018 23:59** для всех групп.\n",
    "* В качестве решения задания нужно прислать ноутбук с подробными комментариями (<span style='color:red'> без присланного решения результат контеста не будет засчитан </span>).\n",
    "* <span style='color:red'>Название команды в контесте должно соответствовать шаблону: НомерГруппы_Имя_Фамилия, например, 594_Ivan_Ivanov</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall_<номер_группы>_<фамилия>``, к примеру -- ``ML2018_fall_495_ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер>.ipnb, к примеру`` -- ``ivanov_401_task7.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall Question <Содержание вопроса>``\n",
    "\n",
    "\n",
    "--------\n",
    "- **PS1:** Используются автоматические фильтры, и просто не найдем ваше дз, если вы неаккуратно его подпишите.\n",
    "- **PS2:**  Просроченный дедлайн снижает максимальный вес задания по формуле, указнной на первом семинаре\n",
    "- **PS3:** Допустимы исправление кода предложенного кода ниже, если вы считаете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем LSTM лучше/хуже чем обычная RNN?\n",
    "\n",
    "<Ответ>: Стандартные RNN страдают от затухающих и взрывающихся градиентов. LSTM (Long Short Term Memory) явно разработаны так, чтобы избежать этой проблемы и более приспособлены к запоминанию “long-term dependencies”.\n",
    "\n",
    "**Вопрос 2**:  Выпишите производную $\\frac{d C_{n+1}}{d C_{k}}$ для LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/, объясните формулу, когда производная затухает, когда взрывается?\n",
    "\n",
    "<Ответ>: \n",
    "Из статьи по ссылке:\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\widetilde{C_t}$$\n",
    "\n",
    "$$f_{t} = \\sigma(W_{f} \\cdot [h_{t-1}, x_{t}] + b_{f})$$\n",
    "\n",
    "Искомая производная: \n",
    "$$\\frac{d C_{n+1}}{d C_{k}} = \\prod_{i=k+1}^{n+1} \\frac{d C_i}{d C_{i-1}} = \\prod_{i=k+1}^{n+1} f_i $$\n",
    "\n",
    "$$||\\frac{d C_{n+1}}{d C_{k}}|| \\leq ||f_i|| = || \\sigma(W_{f} \\cdot [h_{t-1}, x_{t}] + b_{f}) || \\leq \n",
    "\\gamma_{\\sigma}$$ \n",
    "Следовательно, \n",
    "\n",
    "$$ ||\\frac{d C_{n+1}}{d C_{k}}|| \\leq \\gamma_{\\sigma}^{n+1 - k} $$\n",
    "\n",
    "Откуда видно, что если константа $\\gamma_{\\sigma}$ очень маленькая, то производная $\\frac{d C_{n+1}}{d C_{k}}$ очень быстро \"затухает\", а если очень большая, то производная очень быстро \"взрывается\".\n",
    "\n",
    "**Вопрос 3**: Зачем нужен TBPTT почему BPTT плох?\n",
    "\n",
    "<Ответ>: BPTT является расширением классического backpropagation (BP) для рекуррентной нейронной сети (RNN). Тем не менее, в BPTT по-прежнему очень сложно правильно распространять градиент через много слоев из-за так называемой проблемой затухания/взрывания градиента. TBPTT сохраняет вычислительные преимущества BPTT, снижая необходимость полного отслеживания всей последовательности данных на каждом шаге.\n",
    "\n",
    "\n",
    "**Вопрос 4**: Как комбинировать рекуррентные и сверточные сети, а главное зачем? Приведите несколько примеров реальных задач.\n",
    "\n",
    "<Ответ>: Один из способов комбинации: последнюю вычисленную матрицу представления из CNN можно подать на вход RNN. \n",
    "Такой способ применяют для получения описания картинок.\n",
    "Еще так как комбинированная модель будет способна изучать пространственную и временную информацию, то это может быть полезно при решении задач на распознавания видео.\n",
    "\n",
    "**Вопрос 5**: Можно ли использовать сверточные сети для классификации текстов? Если нет обоснуйте :D, если да то как? как решить проблему с произвольной длинной входа?\n",
    "\n",
    "<Ответ>: Можно.\n",
    "Модель: Первые слои вставляют слова в низкоразмерные векторы. Следующий слой выполняет свертки по этим векторам слов, используя различные размеры фильтров. Например, сдвигаясь на 3, 4 или 5 слов за раз. Затем результат сверточного слоя мы даем на вход maxpool и получаем длинный feature-вектор и классифицируем результат с помощью слоя softmax.\n",
    "Как можно решить проблему с произвольной длиной входа: идея состоит в том, чтобы иметь динамический вычислительный граф. Под этим подразумеваетсясоздание нескольких сетей с разной глубиной (в зависимости от их размера входа).\n",
    "\n",
    "**Вопрос 6**: Attention - что это такое, где применяют и как? Приведите пример использования на какой-нибудь задаче\n",
    "\n",
    "<Ответ>: \n",
    "Attention - это механизм, который был разработан для повышения производительности  encoder-decoder модели для машинной трансляции.\n",
    "Изначальная проблема этой модели заключается в том, что нейронная сеть должна иметь возможность сжимать всю необходимую информацию исходного предложения в вектор фиксированной длины. Это может затруднить работу нейронной сети с длинными предложениями (сильное сжатие и потеря информации).  Модель с attention'ом каждый раз, когда генерирует слово в переводе, ищет набор позиций в исходном предложении, где сосредоточена наиболее релевантная информация. Затем модель предсказывает целевое слово на основе векторов контекста, связанных с этими исходными позициями, и всех предыдущих сгенерированных переведенных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "* starting at zero points\n",
    "* +2 for describing your iteration path in a report below (compare models).\n",
    "* +2 for correct check questions\n",
    "* +3 (7 total) for 99% accuracy with simple NMT model on __TEST__ dataset\n",
    "* +3 (10 total) for 99% accuracy with attention NMT model on __TEST__ dataset\n",
    "----\n",
    "* tatoeba bonus for accuracy on __TEST__ dataset:\n",
    "    * +2 for report\n",
    "    * 60% (14 total)\n",
    "    * 65% (16 total)\n",
    "    * 70% (18 total)\n",
    "    * 75% (20 total)\n",
    "    \n",
    "## Bonus points\n",
    "\n",
    "Common ways to get bonus points are:\n",
    "* Get higher score, obviously.\n",
    "* Anything special about your NN. For example \"A super-small/fast NN that gets 99%\" gets a bonus.\n",
    "* Any detailed analysis of the results. (attention maps, whatever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# additional packages for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/96/70d8293a839b21133dcb11bb15aec0ed9d2aca9da6a9766a5e21c848615b/Faker-0.8.13-py2.py3-none-any.whl (741kB)\n",
      "\u001b[K    100% |████████████████████████████████| 747kB 4.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/05/d95bda5a2d833be7593ac0d7eee502acf70d05a4d3a93ef474691a55c531/tqdm-4.23.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 13.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: babel in /anaconda3/lib/python3.6/site-packages (2.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /anaconda3/lib/python3.6/site-packages (from faker) (2.6.1)\n",
      "Requirement already satisfied: six>=1.10 in /anaconda3/lib/python3.6/site-packages (from faker) (1.11.0)\n",
      "Collecting text-unidecode==1.2 (from faker)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/42/d717cc2b4520fb09e45b344b1b0b4e81aa672001dd128c180fabc655c341/text_unidecode-1.2-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 11.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=0a in /anaconda3/lib/python3.6/site-packages (from babel) (2017.3)\n",
      "Installing collected packages: text-unidecode, faker, tqdm\n",
      "Successfully installed faker-0.8.13 text-unidecode-1.2 tqdm-4.23.2\n"
     ]
    }
   ],
   "source": [
    "# ! pip install faker tqdm babel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /anaconda3/lib/python3.6/site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.6\n"
     ]
    }
   ],
   "source": [
    "# ! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task - translation\n",
    "\n",
    "The machine translation is old and well-known field in natural language processing. From the 1950s scientists tried to create a model to automatically translate from say French to English. Nowadays it became possible and the attention mechanism takes great part in that. Here the example image with attention map for the neural machine translation of sample phrase:\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab we will concentrate on much simplier task: we will translate from human readable date to machine readable one.\n",
    "\n",
    "To do this we need to get one more concept - Sequence-to-Sequence language modeling.\n",
    "The idea of such architecture is here:\n",
    "<p aling=\"center\">\n",
    "<img src=\"./img/simple_nmt.jpg\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "There is an Embeding layer at the bottom, the RNN in the middle and softmax as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1851,
     "status": "ok",
     "timestamp": 1524243931024,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "z9tGNB0m5bq3",
    "outputId": "125c7891-6c6a-4e7c-fa41-6969a8d85574"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers import multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import merge\n",
    "\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiHiJe835brn"
   },
   "source": [
    "Now we need to generate data. It will be dates in different text formats and in fixed output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Lqr3mM4D5bro"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Pi8idIsn5brw"
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "67qzEjMm5br1"
   },
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.choice([0,1,2])\n",
    "        if case_change == 1:\n",
    "            human_readable = human_readable.upper()\n",
    "        elif case_change == 2:\n",
    "            human_readable = human_readable.lower()\n",
    "        # if case_change == 0, do nothing\n",
    "\n",
    "        machine_readable = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('22 jun 2011', '2011-06-22', datetime.date(2011, 6, 22))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "j9eTZ13P5br6"
   },
   "outputs": [],
   "source": [
    "def create_dataset(n_examples):\n",
    "    \"\"\"\n",
    "        Creates a dataset with n_examples and vocabularies\n",
    "        :n_examples: the number of examples to generate\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_examples)):\n",
    "        h, m, _ = create_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(list(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(list(machine_vocab) + ['<unk>', '<pad>']))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EsQHOc3D5br9"
   },
   "outputs": [],
   "source": [
    "def string_to_int(string, lenght, vocab):\n",
    "    if len(string) > lenght:\n",
    "        string = string[:lenght]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < lenght:\n",
    "        rep += [vocab['<pad>']] * (lenght - len(string))\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MYU62jZi5bsB"
   },
   "outputs": [],
   "source": [
    "def int_to_string(ints, inv_vocab):\n",
    "    return [inv_vocab[i] for i in ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTghggDB5bsE"
   },
   "source": [
    "Actually generating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20224,
     "status": "ok",
     "timestamp": 1524243982638,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "As0XeIDa5bsF",
    "outputId": "8c6db3db-ec03-4707-8348-2008390a4c9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 236855/300000 [00:09<00:02, 23761.39it/s]/anaconda3/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 300000/300000 [00:12<00:00, 23880.61it/s]\n"
     ]
    }
   ],
   "source": [
    "fake.seed(42)\n",
    "random.seed(42)\n",
    "N = int(3e5)\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ITej2gDY5bsN"
   },
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, TIME_STEPS, human_vocab) for i in inputs])\n",
    "# inputs = [string_to_int(i, TIME_STEPS, human_vocab) for i in inputs]\n",
    "# inputs = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), inputs)))\n",
    "targets = [string_to_int(t, TIME_STEPS, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = (\n",
    "    inputs[:int(2e5)], targets[:int(2e5)], \n",
    "    inputs[int(2e5):-int(5e4)], targets[int(2e5):-int(5e4)],  \n",
    "    inputs[-int(5e4):], targets[-int(5e4):], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Simple NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qWo4x0DM5brd"
   },
   "outputs": [],
   "source": [
    "# :good-enouht:\n",
    "ENCODER_UNITS = 96 # change me if u want\n",
    "DECODER_UNITS = 96 # change me if u want\n",
    " # change me if u want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QQD_Dy8_0MSx"
   },
   "outputs": [],
   "source": [
    "# input - [bs; in_time_len]\n",
    "# output - [bs; out_time_len]; out_time_len=10\n",
    "\n",
    "def model_simple_nmt(in_chars, out_chars):\n",
    "    # RNN encoder -> hidden representation -> RNN decode\n",
    "\n",
    "    # your code\n",
    "    # encoder input model\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    embedding = Embedding(in_chars, ENCODER_UNITS)(inputs)\n",
    "    encoder_lstm = LSTM(ENCODER_UNITS)(embedding)\n",
    "    encoder_rv = RepeatVector(TIME_STEPS)(encoder_lstm)\n",
    "    # decoder output model\n",
    "    decoder_lstm = LSTM(DECODER_UNITS, return_sequences=True)(encoder_rv)\n",
    "    outputs = Dense(out_chars, activation='softmax')(decoder_lstm)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3568,
     "status": "ok",
     "timestamp": 1524243831879,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "ztwvgRe35bsJ",
    "outputId": "282819de-eae6-441c-be05-55a79d3eae66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_89 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_46 (Embedding)     (None, 20, 96)            5760      \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 96)                74112     \n",
      "_________________________________________________________________\n",
      "repeat_vector_27 (RepeatVect (None, 20, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 20, 96)            74112     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20, 13)            1261      \n",
      "=================================================================\n",
      "Total params: 155,245\n",
      "Trainable params: 155,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = model_simple_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 838197,
     "status": "ok",
     "timestamp": 1524223757307,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "l_wPSU2t5bsP",
    "outputId": "e809159c-6c8f-44f8-9bf7-5475237d442f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 115s 576us/step - loss: 0.3278 - acc: 0.8821 - val_loss: 0.0325 - val_acc: 0.9909\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 112s 562us/step - loss: 0.0209 - acc: 0.9923 - val_loss: 0.0322 - val_acc: 0.9874\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 114s 568us/step - loss: 0.0170 - acc: 0.9927 - val_loss: 0.0159 - val_acc: 0.9929\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 111s 557us/step - loss: 0.0165 - acc: 0.9928 - val_loss: 0.0159 - val_acc: 0.9929\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 110s 549us/step - loss: 0.0160 - acc: 0.9929 - val_loss: 0.0157 - val_acc: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1853c16630>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(\n",
    "    [X_train], y_train, \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=5, batch_size=64, \n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 12s 241us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015492873653364369, 0.9929559996795654]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate([X_test], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwYByTjJ5bsS"
   },
   "source": [
    "Lets check our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EIMBnjqY5bsS"
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '6 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1524223952363,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "0cSByOaY5bsW",
    "outputId": "bb237fb8-57d9-429a-fbe9-b654c3c1dcc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3 May 1979\n",
      "output: 1979-05-03<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "input: 6 Apr 09\n",
      "output: 2009-04-06<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "input: 20th February 2016\n",
      "output: 2016-02-20<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "input: Wed 10 Jul 2007\n",
      "output: 2007-02-10<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1979-05-03<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '2009-04-06<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '2016-02-20<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '2007-02-10<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_examples(m, human_vocab, inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9dxpHKlg5bsy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: All u need is attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zDW4e6j5brg"
   },
   "source": [
    "Here we use more complex idea that simple seq2seq: we're adding two explicit parts of our network - encoder and decoder (which is applied attention on). The explanatory picture for this idea is below:\n",
    "<p aling=\"center\"><img src=\"https://i.stack.imgur.com/Zwsmz.png\"></p>\n",
    "\n",
    "The lower part of the network is encoding the input to some hidden intermediate representation and the upper part is decoing the hidвen represenataion into some readable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :good-enouht:\n",
    "ENCODER_UNITS = 96 # change me if u want\n",
    "DECODER_UNITS = 96 # change me if u want\n",
    "TIME_STEPS = 20 # change me if u want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='sigmoid')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QQD_Dy8_0MSx"
   },
   "outputs": [],
   "source": [
    "def model_attention_nmt(in_chars, out_chars):\n",
    "    # RNN encoder -> hidden representation -> RNN decoder\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    # encoder input model\n",
    "    inputs = Input(shape=(TIME_STEPS,))    \n",
    "    embedding = Embedding(in_chars, ENCODER_UNITS)(inputs)\n",
    "    \n",
    "    lstm_out = LSTM(ENCODER_UNITS, return_sequences=True)(embedding)\n",
    "    attention_mul = attention_block(lstm_out)\n",
    "    \n",
    "    decoder_lstm = LSTM(DECODER_UNITS, return_sequences=True)(attention_mul)\n",
    "    output = Dense(out_chars, activation='softmax')(decoder_lstm)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3568,
     "status": "ok",
     "timestamp": 1524243831879,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "ztwvgRe35bsJ",
    "outputId": "282819de-eae6-441c-be05-55a79d3eae66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_105 (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_54 (Embedding)        (None, 20, 96)       5760        input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_78 (LSTM)                  (None, 20, 96)       74112       embedding_54[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_16 (Permute)            (None, 96, 20)       0           lstm_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 96, 20)       420         permute_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 96)       0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 20, 96)       0           lstm_78[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_79 (LSTM)                  (None, 20, 96)       74112       multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 20, 13)       1261        lstm_79[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 155,665\n",
      "Trainable params: 155,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "m = model_attention_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 838197,
     "status": "ok",
     "timestamp": 1524223757307,
     "user": {
      "displayName": "Sergey Kolesnikov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110932373046251109968"
     },
     "user_tz": -180
    },
    "id": "l_wPSU2t5bsP",
    "outputId": "e809159c-6c8f-44f8-9bf7-5475237d442f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 143s 716us/step - loss: 0.4228 - acc: 0.8351 - val_loss: 0.1576 - val_acc: 0.9418\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 133s 667us/step - loss: 0.0716 - acc: 0.9749 - val_loss: 0.0325 - val_acc: 0.9884\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 136s 681us/step - loss: 0.0278 - acc: 0.9897 - val_loss: 0.0231 - val_acc: 0.9913\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 135s 676us/step - loss: 0.0200 - acc: 0.9919 - val_loss: 0.0179 - val_acc: 0.9925\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 136s 680us/step - loss: 0.0186 - acc: 0.9922 - val_loss: 0.0170 - val_acc: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1870fa22b0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(\n",
    "    [X_train], y_train, \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=5, batch_size=64, \n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 17s 332us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.017121005363203583, 0.9925040001678467]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate([X_test], y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "\n",
    "## final architectures\n",
    "\n",
    "1) Simple NMT \n",
    "\n",
    "TIME_STEPS = 20\n",
    "\n",
    "ENCODER_UNITS = 96 \n",
    "\n",
    "DECODER_UNITS = 96\n",
    "\n",
    "- embedding \n",
    "- LSTM - кодирование входа\n",
    "- repeatLayer - размножение последнего выхода для передачи в декодер\n",
    "- LSTM - декодирование\n",
    "- dense - получение результата\n",
    "\n",
    "2) NMT with attention\n",
    "\n",
    "TIME_STEPS = 20\n",
    "\n",
    "ENCODER_UNITS = 96 \n",
    "\n",
    "DECODER_UNITS = 96\n",
    "\n",
    "- embedding \n",
    "- LSTM - кодирование входа\n",
    "\n",
    "- attention_block:\n",
    "- - dense  + sigmoid\n",
    "- - multiply - применяем веса ко входу\n",
    "\n",
    "- LSTM - декодирование\n",
    "- dense - получение результата\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## comparison\n",
    "\n",
    "Вторая сеть учится дольше, потому что в ней больше весов. Какого-то буста в аккураси замечено не было, возможно, потому что задача слишком простая. Ну и куда уж дальше 99 процентов бустить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3*: tatoeba - real NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset from http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open ./rus-eng.zip, ./rus-eng.zip.zip or ./rus-eng.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "! unzip ./rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./rus.txt\") as fin:\n",
    "    data = fin.readlines()\n",
    "data = list(map(lambda x: x.replace(\"\\n\", \"\").lower(), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300108"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:int(1e5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom is here to see you.\\tк тебе том пришёл.'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = list(map(lambda x: x.split(\"\\t\")[0], data))\n",
    "target = list(map(lambda x: x.split(\"\\t\")[1], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = set(\"\".join(source).strip())\n",
    "target_vocab = set(\"\".join(target).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = dict(zip(\n",
    "    list(source_vocab) + ['<unk>', '<pad>'], \n",
    "    list(range(len(source_vocab) + 2))))\n",
    "target_vocab = dict(zip(\n",
    "    list(target_vocab) + ['<unk>', '<pad>'], \n",
    "    list(range(len(target_vocab) + 2))))\n",
    "inv_target_vocab = dict(enumerate(list(target_vocab) + ['<unk>', '<pad>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 32\n",
    "ENCODER_UNITS = 256\n",
    "DECODER_UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simple_nmt_tatoeba(in_chars, out_chars):\n",
    "    # RNN encoder -> hidden representation -> RNN decoder\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    # encoder input model\n",
    "    inputs = Input(shape=(TIME_STEPS,))    \n",
    "    embedding = Embedding(in_chars, ENCODER_UNITS)(inputs)\n",
    "    \n",
    "    lstm_out = LSTM(ENCODER_UNITS, return_sequences=True)(embedding)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    \n",
    "    decoder_lstm = LSTM(DECODER_UNITS, return_sequences=True)(attention_mul)\n",
    "    output = Dense(out_chars, activation='softmax')(decoder_lstm)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_95 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_49 (Embedding)        (None, 32, 256)      13824       input_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (None, 32, 256)      525312      embedding_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 256, 32)      0           lstm_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256, 32)      1056        permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 32, 256)      0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 32, 256)      0           lstm_71[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_72 (LSTM)                  (None, 32, 256)      525312      attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 32, 87)       22359       lstm_72[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,087,863\n",
      "Trainable params: 1,087,863\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "m = model_attention_nmt(len(source_vocab), len(target_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([string_to_int(i, TIME_STEPS, source_vocab) for i in source])\n",
    "targets = [string_to_int(t, TIME_STEPS, target_vocab) for t in target]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(target_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.fit(\n",
    "#     [inputs], targets, \n",
    "#     epochs=10, batch_size=64, \n",
    "#     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_example(m, source_vocab, inv_target_vocab, 'hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tatoeba Report\n",
    "\n",
    "* final architectures\n",
    "* comparison\n",
    "* as well as training method and tricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Copy of tf_seq2seq.ipynb",
   "provenance": [
    {
     "file_id": "1Xtn4UTJOeQ999g78urAnPaZgboio8dz5",
     "timestamp": 1524199403566
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
