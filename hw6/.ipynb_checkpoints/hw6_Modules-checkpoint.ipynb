{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>\n",
    "<img src=\"img/bp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially. \n",
    "         \n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "            \n",
    "        Just write a little loop. \n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        y_cur = input\n",
    "        for module in self.modules:\n",
    "            tmp = module.forward(y_cur)\n",
    "            y_cur = tmp\n",
    "        self.output = y_cur\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "             \n",
    "             \n",
    "        !!!\n",
    "                \n",
    "        To each module you need to provide the input, module saw while forward pass, \n",
    "        it is used while computing gradients. \n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
    "        and NOT `input` to this Sequential module.\n",
    "        \n",
    "        !!!\n",
    "        \n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        grad_cur = gradOutput\n",
    "        for i in np.arange(len(self.modules) - 1, 0, -1):\n",
    "            tmp = self.modules[i].backward(self.modules[i - 1].output, grad_cur)\n",
    "            grad_cur = tmp\n",
    "        self.gradInput = self.modules[0].backward(input, grad_cur)\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(3, size=(2, 3))\n",
    "B = np.random.randint(3, size=(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(10, size=(2))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 1],\n",
       "        [2, 0, 1]]), array([[2, 1, 0, 1],\n",
       "        [1, 1, 2, 0],\n",
       "        [2, 2, 2, 2]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,  B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.dot(A, B)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.dot(B.T, A.T)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 6],\n",
       "       [5, 4],\n",
       "       [6, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[403.42879349, 403.42879349],\n",
       "       [148.4131591 ,  54.59815003],\n",
       "       [403.42879349,   7.3890561 ],\n",
       "       [ 20.08553692,  54.59815003]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  9,  8,  7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([806.85758699, 203.01130914, 410.81784959,  74.68368696])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(D), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([806.85758699, 806.85758699, 203.01130914, 203.01130914,\n",
       "       410.81784959, 410.81784959,  74.68368696,  74.68368696])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.sum(np.exp(D), axis=1).T, repeats=[2, 2, 2, 2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.73105858, 0.26894142],\n",
       "       [0.98201379, 0.01798621],\n",
       "       [0.26894142, 0.73105858]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(np.exp(D).T, np.sum(np.exp(D), axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.98201379, 0.26894142],\n",
       "       [0.5       , 0.26894142, 0.01798621, 0.73105858]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(D).T / np.sum(np.exp(D), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (16,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4fbc005afa54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,2) (16,) "
     ]
    }
   ],
   "source": [
    "np.exp(D) / np.repeat(np.sum(np.exp(D), axis=1), repeats=[4, 4, 4, 4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7]],\n",
       "\n",
       "       [[ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(2*2*4).reshape((2,2,4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7]],\n",
       "\n",
       "       [[ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.arange(2*2*4).reshape((2,4,2))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 28,  34],\n",
       "        [ 76,  98]],\n",
       "\n",
       "       [[428, 466],\n",
       "        [604, 658]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(3, size=(3, 2, 3))\n",
    "B = np.random.randint(3, size=(3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1],\n",
       "        [2, 1, 2]],\n",
       "\n",
       "       [[0, 1, 1],\n",
       "        [2, 0, 2]],\n",
       "\n",
       "       [[1, 0, 2],\n",
       "        [1, 0, 1]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 2]],\n",
       "\n",
       "       [[0, 2]],\n",
       "\n",
       "       [[0, 1]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4, 2, 4]],\n",
       "\n",
       "       [[4, 0, 4]],\n",
       "\n",
       "       [[1, 0, 1]]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "n_feats = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5, 0],\n",
       "       [2, 2, 0],\n",
       "       [3, 1, 3],\n",
       "       [3, 2, 3],\n",
       "       [0, 4, 2],\n",
       "       [3, 1, 3]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.random.randint(6, size=(batch_size, n_feats)) # (n_feats, n_feats)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sol1(M):\n",
    "    b = np.zeros((M.shape[0], M.shape[1], M.shape[1]))\n",
    "    diag = np.arange(M.shape[1])\n",
    "    b[:, diag, diag] = M\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.],\n",
       "        [0., 5., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[2., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[3., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 3.]],\n",
       "\n",
       "       [[3., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 3.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 4., 0.],\n",
       "        [0., 0., 2.]],\n",
       "\n",
       "       [[3., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 3.]]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = sol1(output.reshape(6, 3))\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.reshape(batch_size, n_feats, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  5,  0],\n",
       "        [ 5, 25,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 4,  4,  0],\n",
       "        [ 4,  4,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 9,  3,  9],\n",
       "        [ 3,  1,  3],\n",
       "        [ 9,  3,  9]],\n",
       "\n",
       "       [[ 9,  6,  9],\n",
       "        [ 6,  4,  6],\n",
       "        [ 9,  6,  9]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0, 16,  8],\n",
       "        [ 0,  8,  4]],\n",
       "\n",
       "       [[ 9,  3,  9],\n",
       "        [ 3,  1,  3],\n",
       "        [ 9,  3,  9]]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS = np.matmul(output, output.transpose((0, 2, 1)))\n",
    "SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 7, 7],\n",
       "       [2, 7, 9],\n",
       "       [4, 9, 7],\n",
       "       [8, 3, 7],\n",
       "       [5, 0, 1],\n",
       "       [6, 4, 6]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutput = np.random.randint(10, size=(batch_size, n_feats)) # (batch_size, n_feats)\n",
    "gradOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# хотим (batch_size, n_feats, n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,  -5.,   0.],\n",
       "        [ -5., -20.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[ -2.,  -4.,   0.],\n",
       "        [ -4.,  -2.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[ -6.,  -3.,  -9.],\n",
       "        [ -3.,   0.,  -3.],\n",
       "        [ -9.,  -3.,  -6.]],\n",
       "\n",
       "       [[ -6.,  -6.,  -9.],\n",
       "        [ -6.,  -2.,  -6.],\n",
       "        [ -9.,  -6.,  -6.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [  0., -12.,  -8.],\n",
       "        [  0.,  -8.,  -2.]],\n",
       "\n",
       "       [[ -6.,  -3.,  -9.],\n",
       "        [ -3.,   0.,  -3.],\n",
       "        [ -9.,  -3.,  -6.]]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS = (ans - SS)\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -35., -160.,    0.]],\n",
       "\n",
       "       [[ -32.,  -22.,    0.]],\n",
       "\n",
       "       [[-114.,  -33., -105.]],\n",
       "\n",
       "       [[-129.,  -96., -132.]],\n",
       "\n",
       "       [[   0.,   -8.,   -2.]],\n",
       "\n",
       "       [[-102.,  -36., -102.]]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(gradOutput.reshape(batch_size, 1, n_feats), DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -35., -160.,    0.],\n",
       "       [ -32.,  -22.,    0.],\n",
       "       [-114.,  -33., -105.],\n",
       "       [-129.,  -96., -132.],\n",
       "       [   0.,   -8.,   -2.],\n",
       "       [-102.,  -36., -102.]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(gradOutput.reshape(batch_size, 1, n_feats), DS).reshape(batch_size, n_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer.\n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        # Also, you can make b within W, it is easier.\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_in, n_out))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.dot(input, self.W) + self.b # (batch_size, n_out) + (n_out, 1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.dot(gradOutput, W.T) # (batch_size, n_in)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradW = np.dot(input.T, gradOutput) # (n_in, n_out)\n",
    "        self.gradb = np.sum(gradOutput, axis=0)\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is probably the hardest but as others only takes 5 lines of code in total. \n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из ссылки выше:\n",
    "\n",
    "$$S_j= \\frac{e^{a_j}}{ \\sum_{k=1}^{N} e^{a_k}} $$\n",
    "\n",
    "$$\\frac{\\delta S_i}{\\delta a_j} = D_j S_i = S_i(\\delta_{ij} - S_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        tmp =  np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        self.output = np.divide(np.exp(tmp).T, np.sum(np.exp(tmp), axis=1)).T\n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        output = self.output # (batch_size, n_feats), output[i] = S\n",
    "        batch_size = output.shape[0]\n",
    "        n_feats = output.shape[1]\n",
    "        # формируем диагональные матрицы\n",
    "        diag_matrices = np.zeros((output.shape[0], output.shape[1], output.shape[1]))\n",
    "        diag = np.arange(output.shape[1])\n",
    "        diag_matrices[:, diag, diag] = output\n",
    "        # решейпим output, потому что мы хотим трехмерную матрицу\n",
    "        output = output.reshape(batch_size, n_feats, 1)\n",
    "        # дальше формируем матрицу DS -- матрица производных dS/da (нотация выше в markdown'e)\n",
    "        DS = diag_matrices - np.matmul(output, output.transpose((0, 2, 1)))\n",
    "        # умножаем матрицу градиентов, которые к нам пришли, на матрицу текущих градиентов\n",
    "        # и не забудем решейпнуть, мы ведь хотим матрицу c shape'ом (batch_size, n_feats)\n",
    "        np.matmul(gradOutput.reshape(batch_size, 1, n_feats), DS).reshape(batch_size, n_feats)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. \n",
    "\n",
    "This is a very cool regularizer. In fact, when you see your net is overfitting try to add more dropout.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch). When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterions are used to score the models answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        eps = 1e-15 \n",
    "        input_clamp = np.clip(input, eps, 1 - eps)\n",
    "        \n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15) )\n",
    "                \n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
